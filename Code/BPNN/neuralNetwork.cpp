/*
* The information in this file is
* Copyright(c) 2012, Himanshu Singh <91.himanshu@gmail.com>
* and is subject to the terms and conditions of the
* GNU Lesser General Public License Version 2.1
* The license text is available from   
* http://www.gnu.org/licenses/lgpl.html
*/

#include "ProgressTracker.h"
#include "neuralNetwork.h"
#include "bpnn.h"

#include <vector>
#include <string>
#include <QtCore/QString>
#include <fstream>
#include <cmath>
#include <cstdlib>
using std::vector;
using std::string;

// Read data, generated by classificationData plugin, for training the neural network.
bool NeuralNetwork::readData(const string& inputFileName)
{
    std::ifstream inputFile(inputFileName.c_str());
    string dummy;
    inputFile>>dummy; 
    if (dummy != "BEGIN")
    {
        plugin->progress.report("Invalid input file", 0, ERRORS, true);
        return false;
    }
    int numberOfPoints;
    inputFile>>numberOfPoints;
    int dimension;
    inputFile>>dimension;
    int numberOfClasses;    
    inputFile>>numberOfClasses;

    string name;
    int id;
    for (int i = 0; i < numberOfClasses; i++)
    {
        inputFile>>name;
        classNames.push_back(name);
        inputFile>>id;
        idToClass[id] = name;
    }

    int p;
    // 80% train set
    for (p = 1; p <= 80*numberOfPoints/100; p++)
    {
        plugin->progress.report("Reading input data", 100*p/numberOfPoints, NORMAL, true);
        vector<double> point(dimension);
        for (int i = 0; i < dimension; i++)
        {
            inputFile>>point[i];
        }
        inputFile>>id;

        trainSet.push_back(point);
        yTrain.push_back(id);
    }
    // 20% test set
    for (; p <= numberOfPoints; p++)
    {
        plugin->progress.report("Reading input data", 100*p/numberOfPoints, NORMAL, true);
        vector<double> point(dimension);
        for (int i = 0; i < dimension; i++)
        {
            inputFile>>point[i];
        }
        inputFile>>id;

        testSet.push_back(point);
        yTest.push_back(id);
    }

    inputFile>>dummy; 
    if (dummy != "END")
    {
        plugin->progress.report("Invalid input file", 0, ERRORS, true);
        return false;
    }
    return true;
}

bool NeuralNetwork::saveModel(const string& outputModelFileName)
{
    std::ofstream outputModelFile(outputModelFileName.c_str());
    if (outputModelFile.good() == false)
    {
        plugin->progress.report("Invalid output file", 0, ERRORS, true);
        return false;
    }
    // Write classes
    outputModelFile<<classNames.size()<<"\n";
    for (unsigned int id = 1; id <= classNames.size(); id++)
    {
        outputModelFile<<idToClass[id]<<"\t"<<id<<"\n";
    }
    // units in each layer
    outputModelFile<<inputUnits<<"\n";
    outputModelFile<<hiddenUnits<<"\n";
    outputModelFile<<outputUnits<<"\n";
    // mean of all features
    for (int i = 1; i < inputUnits; i++)
    {
        outputModelFile<<mu[i]<<"\t";
    }
    outputModelFile<<mu[inputUnits]<<"\n";
    // stdv of all features
    for (int i = 1; i < inputUnits; i++)
    {
        outputModelFile<<stdv[i]<<"\t";
    }
    outputModelFile<<stdv[inputUnits]<<"\n";
    // input weight
    for (int i = 0; i <= inputUnits; i++)
    {
        for (int j = 0; j <= hiddenUnits; j++)
        {
            outputModelFile<<inputWeight[i][j]<<"\t";
        }
        outputModelFile<<"\n";
    }
    // hidden weight
    for (int i = 0; i <= hiddenUnits; i++)
    {
        for (int j = 0; j <= outputUnits; j++)
        {
            outputModelFile<<hiddenWeight[i][j]<<"\t";
        }
        outputModelFile<<"\n";
    }
    return true;
}

bool NeuralNetwork::readModel(const string& modelFileName)
{
    std::ifstream modelFile(modelFileName.c_str());
    if (modelFile.good() == false)
    {
        plugin->progress.report("Invalid model file", 0, ERRORS, true);
        return false;
    }
    int numberOfClasses;
    // Read the classes
    modelFile>>numberOfClasses;
    for (int i = 0; i < numberOfClasses; i++)
    {
        string name;
        int id;
        modelFile>>name>>id;
        classNames.push_back(name);
        idToClass[id] = name;
    }
    // Read the number of units in each layer.
    modelFile>>inputUnits;
    modelFile>>hiddenUnits;
    modelFile>>outputUnits;
    // read the mean of each feature
    mu.resize(inputUnits + 1);
    for (int i = 1; i <= inputUnits; i++)
    {
        double m;
        modelFile>>m;
        mu[i] = m;
    }
    // read the standard deviation
    stdv.resize(inputUnits + 1);
    for (int i = 1; i <= inputUnits; i++)
    {
        double s;
        modelFile>>s;
        stdv[i] = s;
    }
    // Read the input and hidden layer weights
    for (int i = 0; i <= inputUnits; i++)
    {
        vector<double> x(hiddenUnits + 1);
        for (int j = 0; j <= hiddenUnits; j++)
        {
            modelFile>>x[j];
        }
        inputWeight.push_back(x);
    }

    for (int i = 0; i <= hiddenUnits; i++)
    {
        vector<double> x(outputUnits + 1);
        for (int j = 0; j <= outputUnits; j++)
        {
            modelFile>>x[j];
        }
        hiddenWeight.push_back(x);
    }

    inputActiv.resize(inputUnits + 1);
    inputActiv[0] = 1.0;
    hiddenActiv.resize(hiddenUnits + 1);
    hiddenActiv[0] = 1.0;
    outputActiv.resize(outputUnits + 1);
    outputActiv[0] = 1.0;

    inputWeightDelta.resize(inputUnits + 1);
    for (int i = 0; i <= inputUnits; i++)
    {
        inputWeightDelta[i].resize(hiddenUnits + 1, 0);
    }
    hiddenWeightDelta.resize(hiddenUnits + 1);
    for (int i = 0; i <= hiddenUnits; i++)
    {
        hiddenWeightDelta[i].resize(outputUnits + 1, 0);
    }

    return true;
}

void NeuralNetwork::normalizeFeatures()
{
    unsigned int features = trainSet[0].size();
    mu.resize(features + 1);
    stdv.resize(features + 1);
    // Compute means for all features
    for (unsigned int feature = 1; feature <= features; feature++)
    {
        double mean = 0.0;
        for (unsigned int p = 0; p < trainSet.size(); p++)
        {
            mean += trainSet[p][feature - 1];
        } 
        mean /= trainSet.size();
        mu[feature] = mean;
    }
    // Calculate Standard Deviation of all features
    for (unsigned int feature = 1; feature <= features; feature++)
    {
        double standardDeviation = 0.0;
        for (unsigned int p = 0; p < trainSet.size(); p++)
        {
            standardDeviation += (trainSet[p][feature - 1] - mu[feature])*(trainSet[p][feature - 1] - mu[feature]);
        }
        standardDeviation /= trainSet.size();
        standardDeviation = sqrt(standardDeviation);
        stdv[feature] = standardDeviation;
    }
    // To avoid division by 0
    for (unsigned int feature = 1; feature <= features; feature++)
        if (stdv[feature] == 0)
            stdv[feature] = 1;
}

inline double NeuralNetwork::sigmoid(const double x)
{
    return 1.0/(1.0 + exp(-x));
}

// Derivative of the sigmoid function.
inline double NeuralNetwork::dsigmoid(const double x)
{
    return x*(1.0 - x);
}

// Intialize the network before training
void NeuralNetwork::initialize()
{
    // Set the number of units in each layer of the Network
    // Input units --> dimension of the training set.
    // Hidden units --> equal to the input units.
    // Output units --> equal to the number of classes.
    inputUnits = trainSet[0].size();
    hiddenUnits = inputUnits;
    outputUnits = classNames.size();
    // +1 for bias term.
    inputActiv.resize(inputUnits + 1);
    inputActiv[0] = 1.0;
    hiddenActiv.resize(hiddenUnits + 1);
    hiddenActiv[0] = 1.0;
    outputActiv.resize(outputUnits + 1);
    outputActiv[0] = 1.0;
    target.resize(outputUnits + 1);

    // Randomly initialize the weights randomly between -1.0 to 1.0
    inputWeight.resize(inputUnits + 1);
    for (int i = 0; i <= inputUnits; i++)
    {
        inputWeight[i].resize(hiddenUnits + 1);
        for (int j = 0; j <= hiddenUnits; j++)
        {
            double x = (double)rand()/RAND_MAX;
            inputWeight[i][j] = 2.0*x - 1.0;
        }
    }
    hiddenWeight.resize(hiddenUnits + 1);
    for (int i = 0; i <= hiddenUnits; i++)
    {
        hiddenWeight[i].resize(outputUnits + 1);
        for (int j = 0; j <= outputUnits; j++)
        {
            double x = (double)rand()/RAND_MAX;
            hiddenWeight[i][j] = 2.0*x - 1.0;
        }
    }

    hiddenDelta.resize(outputUnits + 1);
    outputDelta.resize(outputUnits + 1);
    inputWeightDelta.resize(inputUnits + 1);
    for (int i = 0; i <= inputUnits; i++)
    {
        inputWeightDelta[i].resize(hiddenUnits + 1, 0);
    }
    hiddenWeightDelta.resize(hiddenUnits + 1);
    for (int i = 0; i <= hiddenUnits; i++)
    {
        hiddenWeightDelta[i].resize(outputUnits + 1, 0);
    }
}

void NeuralNetwork::feedForward()
{

    for (int j = 1; j <= hiddenUnits; j++)
    {
        double z = 0.0;
        for (int i = 0; i <= inputUnits; i++)
        {
            z += inputActiv[i]*inputWeight[i][j];
        }
        hiddenActiv[j] = sigmoid(z);
    }

    for (int j = 1; j <= outputUnits; j++)
    {
        double z = 0.0;
        for (int i = 0; i <= hiddenUnits; i++)
        {
            z += hiddenActiv[i]*hiddenWeight[i][j];
        }
        outputActiv[j] = sigmoid(z);
    }
}

double NeuralNetwork::backPropagate()
{
    // Compute error terms for output units.
    double error = 0.0;
    for (int i = 1; i <= outputUnits; i++)
    {
        outputDelta[i] = dsigmoid(outputActiv[i])*(target[i] - outputActiv[i]);
        error += fabs(outputDelta[i]);
    }
    // Compute error terms for hidden Units.
    double s = 0.0;
    for (int i = 1; i <= hiddenUnits; i++)
    {
        s = 0.0;
        for (int j = 1; j <= outputUnits; j++)
        {
            s += outputDelta[j]*hiddenWeight[i][j];
        }
        hiddenDelta[i] = dsigmoid(hiddenActiv[i])*s;
        error += fabs(hiddenDelta[i]);
    }
    // Update weights for hidden unit.
    double deltaW = 0.0;
    for (int i = 1; i <= outputUnits; i++)
    {
        for (int j = 0; j <= hiddenUnits; j++)
        {
            deltaW = learningRate*outputDelta[i]*hiddenActiv[j] + momentum*hiddenWeightDelta[j][i];
            hiddenWeight[j][i] += deltaW;
            hiddenWeightDelta[j][i] = deltaW;
        }
    }
    // Update weights for input unit.
    for (int i = 1; i <= hiddenUnits; i++)
    {
        for (int j = 0; j <= inputUnits; j++)
        {
            deltaW = learningRate*hiddenDelta[i]*inputActiv[j] + momentum*inputWeightDelta[j][i];
            inputWeight[j][i] += deltaW;
            inputWeightDelta[j][i] = deltaW;
        }
    }
    return error;
}

bool NeuralNetwork::train()
{
    // Initialize the network
    initialize();

    normalizeFeatures();

    for (int iteration = 1; iteration <= iterations; iteration++)
    {
        double  errorSum = 0.0;
        // Train on all examples in the trainSet
        for (unsigned int p = 0; p < trainSet.size(); p++)
        {

            if (plugin->isAborted() == true)
            {
                plugin->progress.report("Training aborted", 0, ABORT, true);
                return false;
            }

            // plugin->progress.report("Training Neural Network", 100*(iteration-1)/iterations + 100*(p+1)/trainSet.size()/iterations, NORMAL, true);

            // Set the input activations
            for (int i = 0; i < inputUnits; i++)
            {
                inputActiv[i + 1] = (trainSet[p][i] - mu[i + 1])/stdv[i + 1];
            }
            // Set target
            // fot each training example, if the class id is x, then target[x] is set to HIGH and all other classes LOW.
            for (int j = 1; j <= outputUnits; j++)
            {
                target[j] = LOW;
            }
            target[yTrain[p]] = HIGH;

            // set up the activation units
            feedForward();
            // Train network using backpropagation.
            errorSum += backPropagate();
        }
        plugin->progress.report(QString("Error after iteration %1 = %2\n").arg(iteration).arg(errorSum).toStdString(), 100, WARNING, true);
    }
    computeAccuracy();
    return true;
}
// Predict using the network
string NeuralNetwork::predict(vector<double>& toPredict)
{
    if (toPredict.size() != inputUnits)
    {
        return "INVALID";
    }
    for (int i = 1; i <= inputUnits; i++)
    {
        inputActiv[i] = (toPredict[i - 1] - mu[i])/stdv[i];
    }
    // Calculate output activations
    feedForward();

    int id = 0;
    double best = 0;
    for (int i = 1; i <= outputUnits; i++)
    {
        if (outputActiv[i] > best)
        {
            best = outputActiv[i];
            id = i;
        }
    }
    // If no class matches
    if (id == 0 || best < 0.5)
    {
        return "UNKNOWN";
    }
    return idToClass[id];
}

void NeuralNetwork::computeAccuracy()
{
    int errors = 0.0;
    for (unsigned int p = 0; p < trainSet.size(); p++)
    {
        plugin->progress.report("Computing accuracy on train set", 100*(p+1)/trainSet.size(), NORMAL, true);
        string prediction = predict(trainSet[p]);
        if (prediction != idToClass[yTrain[p]])
        {
            errors++;
        }
    }
    double errorRate = 100*(double)errors/trainSet.size();
    plugin->progress.report(QString("Train error = %1\n").arg(errorRate).toStdString(), 100, WARNING, true);

    errors = 0;
    for (unsigned int p = 0; p < testSet.size(); p++)
    {
        plugin->progress.report("Computing accuracy on test set", 100*(p+1)/testSet.size(), NORMAL, true);
        string prediction = predict(testSet[p]);
        if (prediction != idToClass[yTest[p]])
        {
            errors++;
        }
    }
    errorRate = 100*(double)errors/testSet.size();
    plugin->progress.report(QString("Test error = %1\n").arg(errorRate).toStdString(), 100, WARNING, true);
}
